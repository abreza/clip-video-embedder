{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WDMuyJ3zMs1",
        "outputId": "6640a9c8-3da1-4b86-ed61-670bdf9efcaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/clip-video-embedder\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "from IPython.display import clear_output\n",
        "\n",
        "if not os.path.exists('/content/clip-video-embedder'):\n",
        "    !git clone -b dev https://github.com/abreza/clip-video-embedder.git\n",
        "    %cd /content/clip-video-embedder\n",
        "    !pip install -r requirements.txt\n",
        "    clear_output()\n",
        "\n",
        "%cd /content/clip-video-embedder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHvNQBcucrNG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "import math\n",
        "import cv2\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3RtdVeESrhU"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_name):\n",
        "\n",
        "    json_file = 'train' if dataset_name == 'ActivityNet Captions' else 'activity_net.v1-3.min'\n",
        "    data_path = f'datasets/ActivityNet/{json_file}.json'\n",
        "\n",
        "    with open(data_path) as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "    if dataset_name == 'ActivityNet':\n",
        "      data = data[\"database\"]\n",
        "      data = {id : data[id] for id in list(data.keys()) if data[id]['subset'] == 'training'}\n",
        "\n",
        "    return data\n",
        " \n",
        "\n",
        "def load_video_informations(video_id):\n",
        "\n",
        "    data = load_dataset('ActivityNet Captions')\n",
        "    duration = data[video_id][\"duration\"]\n",
        "    sentences = data[video_id][\"sentences\"]\n",
        "    timestamps = data[video_id][\"timestamps\"]\n",
        "\n",
        "    data = load_dataset('ActivityNet')\n",
        "    corresponding_label = data[video_id[2:]][\"annotations\"][0][\"label\"] \n",
        "\n",
        "    return sentences, timestamps, duration, corresponding_label\n",
        "\n",
        "\n",
        "def choice_video(data, length, var=10):\n",
        "    duration = -1\n",
        "    video_ids = list(data.keys())\n",
        "\n",
        "    min_length = length - var\n",
        "    max_length = length + var\n",
        "\n",
        "    while not(duration > min_length and duration < max_length):\n",
        "        id = random.choice(video_ids)\n",
        "        duration = data[id][\"duration\"]\n",
        "\n",
        "    return id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ActivityNet Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3wfi4pzECW3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "activityNet_train_path = 'datasets/ActivityNet/train.json'\n",
        "\n",
        "with open(activityNet_train_path) as f:\n",
        "  data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiBZoPJ2r83D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from utils.video_loader import download_video_from_youtube\n",
        "from dataloaders.rawvideo_util import RawVideoExtractor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aghDcAu-3llN"
      },
      "outputs": [],
      "source": [
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).eval()\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW1QKgsO1bdX"
      },
      "outputs": [],
      "source": [
        "def frame_sampler(video_path, framerate):\n",
        "    video_extractor = RawVideoExtractor(framerate=framerate)\n",
        "    frames = video_extractor.get_video_data(video_path)['video'].to(device)\n",
        "    return frames\n",
        "\n",
        "def get_video_dimensions(video_path):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = str(int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)))\n",
        "    height = str(int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    return (width, height)\n",
        "\n",
        "def choice_video(data, length, var=10):\n",
        "    duration = -1\n",
        "    video_ids = list(data.keys())\n",
        "\n",
        "    min_length = length - var\n",
        "    max_length = length + var\n",
        "\n",
        "    while not(duration > min_length and duration < max_length):\n",
        "        id = random.choice(video_ids)\n",
        "        duration = data[id][\"duration\"]\n",
        "\n",
        "    return id\n",
        "\n",
        "def extract_frames(video_path, framerate):\n",
        "    tensors = frame_sampler(video_path, framerate)\n",
        "    tensors = pad_sequence(tensors, batch_first=True, padding_value=0)\n",
        "    return tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijXSeG7_vaEH"
      },
      "outputs": [],
      "source": [
        "def trim_sentences(sentences, max_seq_len=77):\n",
        "\n",
        "    trimmed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.strip().split()\n",
        "\n",
        "        # Trim the sentence if it is too long\n",
        "        if len(tokens) > max_seq_len:\n",
        "            tokens = tokens[:max_seq_len]\n",
        "\n",
        "        # Join the tokens back into a string\n",
        "        trimmed_sentence = \" \".join(tokens)\n",
        "\n",
        "        trimmed_sentences.append(trimmed_sentence)\n",
        "\n",
        "    return trimmed_sentences\n",
        "\n",
        "def truncate_sentence(sentence, max_display_length=120):\n",
        "    truncated_sentence = sentence.strip()\n",
        "    if len(truncated_sentence) > max_display_length:\n",
        "        truncated_sentence = truncated_sentence[:max_display_length] + \" ...\"\n",
        "    else:\n",
        "        truncated_sentence = truncated_sentence\n",
        "    return truncated_sentence\n",
        "\n",
        "def generate_clippable_text(sentences, corresponding_label, random_sentences=None):\n",
        "    all_sentences = [' '.join(sentences), corresponding_label, f\"It's a video of {corresponding_label}.\"] + sentences\n",
        "    all_sentences = all_sentences + random_sentences if random_sentences else all_sentences\n",
        "    all_sentences = trim_sentences(all_sentences,  max_seq_len=67)\n",
        "    return all_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMVogtVPLvjS"
      },
      "outputs": [],
      "source": [
        "def plot_video_frames(video_path, frames):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames_data = []\n",
        "    for frame_num in frames:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
        "        _, frame = cap.read()\n",
        "        frames_data.append(frame)\n",
        "    cap.release()\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "    for i in range(4):\n",
        "        axs[i].imshow(frames_data[i])\n",
        "        axs[i].set_title(f\"Frame {frames[i]}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM1FoaEi1twO"
      },
      "outputs": [],
      "source": [
        "def print_line(length=75):\n",
        "  print('-'*length)\n",
        "\n",
        "\n",
        "def print_sentences(corresponding_label, sentences, random_sentences=None):\n",
        "\n",
        "    print_line()\n",
        "    print(f\"Label: {corresponding_label}\")\n",
        "    print_line()\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"Description {i+1:2d}: {truncate_sentence(sentence)}\")\n",
        "\n",
        "    if random_sentences:\n",
        "      print_line()\n",
        "      for i, random_sent in enumerate(random_sentences):\n",
        "        print(f'Random Description {i+1:2d}: {truncate_sentence(random_sent)}')\n",
        "\n",
        "    print_line()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fgg1JLyb0Uqw"
      },
      "outputs": [],
      "source": [
        "def plot_list_of_lists(outputs, sentences, random_sentences=None,\n",
        "                       timestamps=[], framerate=1, max_display_length=100,\n",
        "                       force_separate_subplots = False,\n",
        "                       show_random_plots = False,\n",
        "                       show_corresponding_label_plot=False,\n",
        "                       show_prompt_label_plot = False,\n",
        "                       show_concate_descriptions_plot=False,\n",
        "                       show_average_plot = False, \n",
        "                       show_each_plot=False):\n",
        "           \n",
        "    concat_output = outputs[0].tolist()\n",
        "    corresponding_label_output = outputs[1].tolist()\n",
        "    prompt_label_output = outputs[2].tolist()\n",
        "    list_of_lists = np.array([output.tolist() for output in outputs[3:]])\n",
        "\n",
        "    t = np.arange(outputs.shape[1])\n",
        "\n",
        "    _temp = len(t)/framerate\n",
        "    interval_length = 1 if _temp<25 else 2 if _temp < 50 else 5 if _temp < 100 else 10 if _temp <260 else 10 if _temp<360 else 20\n",
        "    plot_w = 12 if _temp<150 else 18 if _temp<350 else 22\n",
        "\n",
        "\n",
        "    if force_separate_subplots:\n",
        "\n",
        "        number_of_subplots = len(sentences)+len(random_sentences) if random_sentences and show_random_plots else len(sentences)\n",
        "\n",
        "        fig , axs = plt.subplots(number_of_subplots, 1, figsize=(plot_w, number_of_subplots*2))\n",
        "\n",
        "        for i,rasentence in enumerate(sentences):\n",
        "          axs[i].plot(t/ framerate, list_of_lists[i])\n",
        "          axs[i].set_title(truncate_sentence(sentences[i]))\n",
        "          axs[i].set_xlabel(\"Time (s)\")\n",
        "          axs[i].set_ylabel(\"Similarity\")\n",
        "          axs[i].axvline(x=timestamps[i][0] , color='r', linestyle='--')\n",
        "          axs[i].axvline(x=timestamps[i][1] , color='r', linestyle='--')\n",
        "          axs[i].set_xticks(np.arange(0, round(len(t)/ framerate) + 1, interval_length))\n",
        "\n",
        "\n",
        "        if random_sentences and show_random_plots:        \n",
        "          for i, random_sent in enumerate(random_sentences):\n",
        "            index = i + len(sentences)\n",
        "            axs[index].plot(t/ framerate, list_of_lists[index], color='orange')\n",
        "            axs[index].set_title(truncate_sentence('Random Sentence: '+random_sent))\n",
        "            axs[index].set_xlabel(\"Time (s)\")\n",
        "            axs[index].set_ylabel(\"similarity\")\n",
        "            axs[index].set_xticks(np.arange(0, round(len(t)/ framerate) + 1, interval_length))\n",
        "\n",
        "\n",
        "    else:\n",
        "        \n",
        "        fig , ax = plt.subplots(1, 1, figsize=(plot_w, 4))\n",
        "\n",
        "        if show_concate_descriptions_plot:\n",
        "            ax.plot(t/ framerate, concat_output, label='Concatenated Descriptions')\n",
        "\n",
        "        if show_corresponding_label_plot:\n",
        "            ax.plot(t/ framerate, corresponding_label_output, label='Corresponding Label')\n",
        "\n",
        "        if show_prompt_label_plot:\n",
        "            ax.plot(t/ framerate, prompt_label_output, label=\"Prompt: It's a video of {Label}\")\n",
        "\n",
        "        if show_average_plot:\n",
        "            mean_of_lists = np.mean(list_of_lists[:len(sentences)], axis=0)\n",
        "            ax.plot(t/ framerate, mean_of_lists, label='Average Plot')\n",
        "        \n",
        "        if show_each_plot:\n",
        "            for i in range(len(sentences)):\n",
        "                ax.plot(t/ framerate, list_of_lists[i], label=f'Description {i+1}')\n",
        "\n",
        "        if random_sentences and show_random_plots:\n",
        "          if len(random_sentences) == 1:\n",
        "            ax.plot(t/ framerate, list_of_lists[len(sentences)], label=f'Random Description', linestyle='--')\n",
        "          \n",
        "          else:  \n",
        "            for i in range(len(random_sentences)):\n",
        "              index = len(sentences)+i\n",
        "              ax.plot(t/ framerate, list_of_lists[index], label=f'Random Description {i+1}', linestyle='--')\n",
        "\n",
        "        ax.set_title(\"CLIP Text-Frame Cosine Similarity\")\n",
        "        ax.set_xlabel(\"Time (s)\")\n",
        "        ax.set_ylabel(\"text-frame similarity\")\n",
        "        ax.set_xticks(np.arange(0, round(len(t)/ framerate) + 1, interval_length))\n",
        "        ax.legend()\n",
        "\n",
        "    # if save_plots: fig.savefig(f\"{video_id} - {framerate}fps.png\")\n",
        "\n",
        "    fig.subplots_adjust(hspace=1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3cVAxMX_Xuw"
      },
      "outputs": [],
      "source": [
        "def clip_inference(sentences, tensors):\n",
        "    \n",
        "    inputs = processor(text=sentences, images=tensors, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    new_inputs = {}\n",
        "    for key in list(inputs.keys()):\n",
        "        new_inputs[key] = inputs[key].to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**new_inputs)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPWxvaHl9Tfp"
      },
      "outputs": [],
      "source": [
        "def find_available_video(data, length):\n",
        "  number_of_failure = 0\n",
        "  while True:\n",
        "    try:\n",
        "        video_id = choice_video(data, length)\n",
        "        sentences, _, duration, corresponding_label = load_video_informations(video_id)\n",
        "        video_path = download_video_from_youtube(video_id[2:], f'./videos/')\n",
        "        print(f'Number of Failure: {number_of_failure}')\n",
        "        print_line()\n",
        "        return video_id, video_path\n",
        "    except Exception as e:\n",
        "        number_of_failure += 1\n",
        "        clear_output()\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yTCZmE1r255"
      },
      "outputs": [],
      "source": [
        "def summarize_settings(settings):\n",
        "    if settings.get('force_separate_subplots', False):\n",
        "        return \"multiple\"\n",
        "    \n",
        "    summary = []\n",
        "    \n",
        "    if settings.get('show_random_plots', False):\n",
        "        summary.append(\"random\")\n",
        "        \n",
        "    if settings.get('show_corresponding_label_plot', False):\n",
        "        summary.append(\"label\")\n",
        "        \n",
        "    if settings.get('show_prompt_label_plot', False):\n",
        "        summary.append(\"prompt\")\n",
        "        \n",
        "    if settings.get('show_concate_descriptions_plot', False):\n",
        "        summary.append(\"concat\")\n",
        "        \n",
        "    if settings.get('show_average_plot', False):\n",
        "        summary.append(\"avg\")\n",
        "        \n",
        "    if settings.get('show_each_plot', False):\n",
        "        summary.append(\"all\")\n",
        "    \n",
        "    return \"single({})\".format(\"+\".join(summary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB3RwGE-HThB"
      },
      "outputs": [],
      "source": [
        "def video_analysis(length, random_sentences = None, framerate=1):\n",
        "\n",
        "    video_id, video_path = find_available_video(data, length=length)\n",
        "\n",
        "    sentences, timestamps, duration, corresponding_label = load_video_informations(video_id)\n",
        "\n",
        "    _dimension = \"x\".join(get_video_dimensions(video_path))\n",
        "    print(f'ID: {video_id} | Duration: {duration:.2f} sec | {_dimension} | #sent: {len(sentences):2d} | fps: {framerate}',end='')\n",
        "              \n",
        "    tic = time.time()\n",
        "    tensors = extract_frames(video_path, framerate)\n",
        "    print(f' | Frame extraction: {time.time()-tic :2.2f} sec',end='')\n",
        "\n",
        "    tic = time.time()\n",
        "    outputs = clip_inference(generate_clippable_text(sentences, corresponding_label, random_sentences), tensors)\n",
        "    outputs = outputs.logits_per_text.detach().cpu().numpy()\n",
        "    \n",
        "    print(f' | CLIP inference ({device}): {time.time()-tic :2.2f} sec')\n",
        "    \n",
        "    print_sentences(corresponding_label, sentences, random_sentences)\n",
        "\n",
        "    return outputs, video_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ePWI-CJYBF-",
        "outputId": "65f25d8d-28a1-4612-f2c9-916fcf2066ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading YouTube video 8z8FprjMNbI.\n",
            "Download complete.\n",
            "Number of Failure: 14\n",
            "---------------------------------------------------------------------------\n",
            "ID: v_8z8FprjMNbI | Duration: 332.45 sec | 568x320 | #sent:  3 | fps: 10"
          ]
        }
      ],
      "source": [
        "random_sentences = ['None']\n",
        "framerate = 10\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "outputs, video_id = video_analysis(length=329, random_sentences = random_sentences, framerate = framerate)\n",
        "\n",
        "setting = {\n",
        "           'force_separate_subplots'  : True, #Disables all of the followings if be True\n",
        "           'show_random_plots': True,\n",
        "           'show_corresponding_label_plot' : True,\n",
        "           'show_prompt_label_plot' : True,\n",
        "           'show_concate_descriptions_plot': True,\n",
        "           'show_average_plot':True,\n",
        "           'show_each_plot': True,\n",
        "           }\n",
        "\n",
        "sentences, timestamps, duration, corresponding_label = load_video_informations(video_id)\n",
        "\n",
        "# print_sentences(corresponding_label, sentences, random_sentences)\n",
        "plot_list_of_lists(outputs, sentences, random_sentences, timestamps, framerate=framerate, **setting)\n",
        "\n",
        "print(f\"{video_id[2:]}-{framerate}fps-{summarize_settings(setting)}.png\")\n",
        "print(f'\\nURL: https://www.youtube.com/watch?v={video_id[2:]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8essnejcLdK"
      },
      "source": [
        "# Statistical analysis of the data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1me_TSOrOwA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def compute_size(tensor_shape):\n",
        "    size = math.prod(tensor_shape) * 4 / 1024/1024\n",
        "\n",
        "    if size/1024 <1:\n",
        "      print(f\"Size: {size:.2f} MB\")\n",
        "    else:\n",
        "      print(f\"Size: {size/1024:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EV4g2cJr2CV",
        "outputId": "64261924-37d6-4197-d91c-b623a821476e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size: 574.22 MB\n"
          ]
        }
      ],
      "source": [
        "compute_size((224,224,3,200,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMWy6JBWe0nl"
      },
      "outputs": [],
      "source": [
        "data_path = f'datasets/ActivityNet/activity_net.v1-3.min.json'\n",
        "\n",
        "with open(data_path) as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "data = data[\"taxonomy\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe4ZKfzmcN9J"
      },
      "outputs": [],
      "source": [
        "def build_tree(nodes, parent_id=None):\n",
        "    # Filter nodes with matching parent_id\n",
        "    children = [node for node in nodes if node['parentId'] == parent_id]\n",
        "\n",
        "    # Sort children by nodeName\n",
        "    children.sort(key=lambda x: x['nodeName'])\n",
        "\n",
        "    # Recursively build tree for each child\n",
        "    tree = {}\n",
        "    for child in children:\n",
        "        tree[child['nodeName']] = build_tree(nodes, child['nodeId'])\n",
        "\n",
        "    return tree\n",
        "\n",
        "def print_tree(tree, level=0, indicators=['-','○','+','•']):\n",
        "  # Iterate over tree\n",
        "  for i, (name, child) in enumerate(tree.items()):\n",
        "    # Select indicator based on level\n",
        "    indicator = indicators[level % len(indicators)]\n",
        "\n",
        "    # Print node with indicator and level\n",
        "    print(\"  \" * level + f\"{indicator} {name}\")\n",
        "\n",
        "    # Recursively print child nodes\n",
        "    if child:\n",
        "        print_tree(child, level + 1, indicators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbj494yU0KQL"
      },
      "outputs": [],
      "source": [
        "def get_min_max_time_segments(segments):\n",
        "    start_times = [segment[0] for segment in segments]\n",
        "    end_times = [segment[1] for segment in segments]\n",
        "    min_start_time = min(start_times)\n",
        "    max_end_time = max(end_times)\n",
        "    return [min_start_time, max_end_time]\n",
        "\n",
        "def get_total_length(segments):\n",
        "    return sum([segment[1]-segment[0] for segment in segments])\n",
        "\n",
        "def get_ratio(min_max_array,total_length):\n",
        "    return (min_max_array[1]-min_max_array[0])/total_length"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "G8essnejcLdK"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "9f8f6ac5df62d4c010a470c096af89c29157a9d743a2d25d86d4ae7e90eff14c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
