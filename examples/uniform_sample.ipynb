{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/abreza/clip-video-embedder\n",
        "%cd clip-video-embedder/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.video_loader import download_video_from_youtube\n",
        "\n",
        "video_id = \"4uw4co69JUQ\"\n",
        "video_path = download_video_from_youtube('4uw4co69JUQ', './videos')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"clip-vit-large-patch14\")\n",
        "processor = CLIPProcessor.from_pretrained(\"clip-vit-large-patch14\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from modules.frame_sampler.uniform_sampler import uniform_sample_frames\n",
        "from modules.frame_sampler.aggregate_embedding import aggregate_embeddings\n",
        "\n",
        "num_frames = 10\n",
        "video_frames = uniform_sample_frames(video_path, num_frames)\n",
        "\n",
        "frame_embeddings = []\n",
        "for frame in video_frames:\n",
        "    inputs = processor(images=frame, return_tensors=\"pt\")\n",
        "    outputs = model.get_image_features(**inputs)\n",
        "    frame_embedding = outputs.detach().numpy()\n",
        "    frame_embeddings.append(frame_embedding)\n",
        "\n",
        "video_embedding = aggregate_embeddings(frame_embeddings, strategy=\"mean\")\n",
        "\n",
        "# Compare the video embedding to a given text\n",
        "texts = [\"a dog playing in the park\", \"playing basketball\"]\n",
        "for text in texts:\n",
        "  text_inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n",
        "  text_outputs = model.get_text_features(**text_inputs)\n",
        "  text_embedding = text_outputs.detach().numpy()\n",
        "  \n",
        "  similarity = np.inner(video_embedding, text_embedding)\n",
        "  print(text, similarity)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
